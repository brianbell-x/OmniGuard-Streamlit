# Responses API Usage


## o4-mini Usage

```python
from openai import OpenAI
client = OpenAI()

response = client.responses.create(
  model="o4-mini",
  input=[
    {
      "role": "developer",
      "content": [
        {
          "type": "input_text",
          "text": "{{developer/system_message}}"
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "input_text",
          "text": "{{user_message}}"
        }
      ]
    }
  ],
  text={
    "format": {
      "type": "json_object"
    }
  },
  reasoning={
    "effort": "medium",
    "summary": "detailed"
  },
  tools=[],
  store=True
)
```

## gpt-4o-mini Usage

```python
from openai import OpenAI
client = OpenAI()

response = client.responses.create(
  model="gpt-4o-mini",
  input=[
    {
      "role": "system",
      "content": [
        {
          "type": "input_text",
          "text": "{{developer/system_message}}"
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "input_text",
          "text": "{{user_message}}"
        }
      ]
    }
  ],
  text={
    "format": {
      "type": "text"
    }
  },
  reasoning={},
  tools=[],
  temperature=1,
  max_output_tokens=10000,
  top_p=1,
  store=True
)
```

## Full API Reference

### Create a Model Response

**POST**  
`https://api.openai.com/v1/responses`

Creates a model response. Provide text or image inputs to generate text or JSON outputs. Have the model call your own custom code or use built-in tools like web search or file search to use your own data as input for the model's response.

---

### Request Body

#### `input` (string or array) **Required**
Text, image, or file inputs to the model, used to generate a response.

**Learn more:**
- [Text inputs and outputs](#)
- [Image inputs](#)
- [File inputs](#)
- [Conversation state](#)
- [Function calling](#)

---

#### Input Types

- **Text input** (`string`):  
  A text input to the model, equivalent to a text input with the user role.

- **Input item list** (`array`):  
  A list of one or many input items to the model, containing different content types.

---

#### Input Message (`object`)
A message input to the model with a role indicating instruction following hierarchy. Instructions given with the developer or system role take precedence over instructions given with the user role. Messages with the assistant role are presumed to have been generated by the model in previous interactions.

**Properties:**
- `content` (`string` or `array`) **Required**  
  Text, image, or audio input to the model, used to generate a response. Can also contain previous assistant responses.

---

##### Input Item Content List (`array`)
A list of one or many input items to the model, containing different content types.

---

##### Input Text (`object`)
A text input to the model.

**Properties:**
- `text` (`string`) **Required**  
  The text input to the model.
- `type` (`string`) **Required**  
  The type of the input item. Always `input_text`.

---

##### Input Image (`object`)
An image input to the model. [Learn about image inputs](#).

**Properties:**
- `detail` (`string`) **Required**  
  The detail level of the image to be sent to the model. One of `high`, `low`, or `auto`. Defaults to `auto`.
- `type` (`string`) **Required**  
  The type of the input item. Always `input_image`.
- `file_id` (`string`) *Optional*  
  The ID of the file to be sent to the model.
- `image_url` (`string`) *Optional*  
  The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL.

---

##### Input File (`object`)
A file input to the model.

**Properties:**
- `type` (`string`) **Required**  
  The type of the input item. Always `input_file`.
- `file_data` (`string`) *Optional*  
  The content of the file to be sent to the model.
- `file_id` (`string`) *Optional*  
  The ID of the file to be sent to the model.
- `filename` (`string`) *Optional*  
  The name of the file to be sent to the model.

---

#### Message Input Properties

- `role` (`string`) **Required**  
  The role of the message input. One of `user`, `assistant`, `system`, or `developer`.
- `type` (`string`) *Optional*  
  The type of the message input. Always `message`.

---

#### Item (`object`)
An item representing part of the context for the response to be generated by the model. Can contain text, images, and audio inputs, as well as previous assistant responses and tool call outputs.

---

#### Output Message (`object`)
An output message from the model.

**Properties:**
- `content` (`array`) **Required**  
  The content of the output message.

---

##### Output Text (`object`)
A text output from the model.

**Properties:**
- `annotations` (`array`) **Required**  
  The annotations of the text output.

---

###### File Citation (`object`)
A citation to a file.

**Properties:**
- `file_id` (`string`) **Required**  
  The ID of the file.
- `index` (`integer`) **Required**  
  The index of the file in the list of files.
- `type` (`string`) **Required**  
  The type of the file citation. Always `file_citation`.

---

###### URL Citation (`object`)
A citation for a web resource used to generate a model response.

**Properties:**
- `end_index` (`integer`) **Required**  
  The index of the last character of the URL citation in the message.
- `start_index` (`integer`) **Required**  
  The index of the first character of the URL citation in the message.
- `title` (`string`) **Required**  
  The title of the web resource.
- `type` (`string`) **Required**  
  The type of the URL citation. Always `url_citation`.
- `url` (`string`) **Required**  
  The URL of the web resource.

---

###### File Path (`object`)
A path to a file.

**Properties:**
- `file_id` (`string`) **Required**  
  The ID of the file.
- `index` (`integer`) **Required**  
  The index of the file in the list of files.
- `type` (`string`) **Required**  
  The type of the file path. Always `file_path`.

---

- `text` (`string`) **Required**  
  The text output from the model.
- `type` (`string`) **Required**  
  The type of the output text. Always `output_text`.

---

##### Refusal (`object`)
A refusal from the model.

**Properties:**
- `refusal` (`string`) **Required**  
  The refusal explanation from the model.
- `type` (`string`) **Required**  
  The type of the refusal. Always `refusal`.

---

- `id` (`string`) **Required**  
  The unique ID of the output message.
- `role` (`string`) **Required**  
  The role of the output message. Always `assistant`.
- `status` (`string`) **Required**  
  The status of the message input. One of `in_progress`, `completed`, or `incomplete`. Populated when input items are returned via API.
- `type` (`string`) **Required**  
  The type of the output message. Always `message`.

---

#### Reasoning (`object`)
A description of the chain of thought used by a reasoning model while generating a response. Be sure to include these items in your input to the Responses API for subsequent turns of a conversation if you are manually managing context.

**Properties:**
- `id` (`string`) **Required**  
  The unique identifier of the reasoning content.
- `summary` (`array`) **Required**  
  Reasoning text contents.

  - `text` (`string`) **Required**  
    A short summary of the reasoning used by the model when generating the response.
  - `type` (`string`) **Required**  
    The type of the object. Always `summary_text`.

- `type` (`string`) **Required**  
  The type of the object. Always `reasoning`.
- `encrypted_content` (`string` or `null`) *Optional*  
  The encrypted content of the reasoning item - populated when a response is generated with `reasoning.encrypted_content` in the include parameter.
- `status` (`string`) *Optional*  
  The status of the item. One of `in_progress`, `completed`, or `incomplete`. Populated when items are returned via API.

---

#### Item Reference (`object`)
An internal identifier for an item to reference.

**Properties:**
- `id` (`string`) **Required**  
  The ID of the item to reference.
- `type` (`string`) *Optional*  
  Defaults to `item_reference`. The type of item to reference. Always `item_reference`.

---

#### Other Parameters

- `model` (`string`) **Required**  
  Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI offers a wide range of models with different capabilities, performance characteristics, and price points. Refer to the model guide to browse and compare available models.

- `include` (`array` or `null`) *Optional*  
  Specify additional output data to include in the model response. Currently supported values are:
  - `file_search_call.results`: Include the search results of the file search tool call.
  - `message.input_image.image_url`: Include image URLs from the input message.
  - `computer_call_output.output.image_url`: Include image URLs from the computer call output.
  - `reasoning.encrypted_content`: Includes an encrypted version of reasoning tokens in reasoning item outputs. This enables reasoning items to be used in multi-turn conversations when using the Responses API statelessly (like when the store parameter is set to false, or when an organization is enrolled in the zero data retention program).

- `instructions` (`string` or `null`) *Optional*  
  Inserts a system (or developer) message as the first item in the model's context.  
  When using along with `previous_response_id`, the instructions from a previous response will not be carried over to the next response. This makes it simple to swap out system (or developer) messages in new responses.

- `max_output_tokens` (`integer` or `null`) *Optional*  
  An upper bound for the number of tokens that can be generated for a response, including visible output tokens and reasoning tokens.

- `metadata` (`map`) *Optional*  
  Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.  
  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters.

- `parallel_tool_calls` (`boolean` or `null`) *Optional*  
  Defaults to `true`. Whether to allow the model to run tool calls in parallel.

- `previous_response_id` (`string` or `null`) *Optional*  
  The unique ID of the previous response to the model. Use this to create multi-turn conversations. [Learn more about conversation state](#).

- `reasoning` (`object` or `null`) *Optional*  
  *o-series models only*  
  Configuration options for reasoning models.

  - `effort` (`string` or `null`) *Optional*  
    Defaults to `medium`.  
    *o-series models only*  
    Constrains effort on reasoning for reasoning models. Currently supported values are `low`, `medium`, and `high`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.

  - `generate_summary` (**Deprecated**) (`string` or `null`) *Optional*  
    Deprecated: use `summary` instead.  
    A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's reasoning process. One of `auto`, `concise`, or `detailed`.

  - `summary` (`string` or `null`) *Optional*  
    A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's reasoning process. One of `auto`, `concise`, or `detailed`.

- `service_tier` (`string` or `null`) *Optional*  
  Defaults to `auto`.  
  Specifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:
  - If set to `auto`, and the Project is Scale tier enabled, the system will utilize scale tier credits until they are exhausted.
  - If set to `auto`, and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarantee.
  - If set to `default`, the request will be processed using the default service tier with a lower uptime SLA and no latency guarantee.
  - If set to `flex`, the request will be processed with the Flex Processing service tier. [Learn more](#).
  - When not set, the default behavior is `auto`.
  - When this parameter is set, the response body will include the `service_tier` utilized.

- `store` (`boolean` or `null`) *Optional*  
  Defaults to `true`.  
  Whether to store the generated model response for later retrieval via API.

- `stream` (`boolean` or `null`) *Optional*  
  Defaults to `false`.  
  If set to `true`, the model response data will be streamed to the client as it is generated using server-sent events. See the [Streaming section](#) below for more information.

- `temperature` (`number` or `null`) *Optional*  
  Defaults to `1`.  
  What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both.

- `text` (`object`) *Optional*  
  Configuration options for a text response from the model. Can be plain text or structured JSON data.  
  [Learn more: Text inputs and outputs](#) | [Structured Outputs](#)

  - `format` (`object`) *Optional*  
    An object specifying the format that the model must output.
    - Configuring `{ "type": "json_schema" }` enables Structured Outputs, which ensures the model will match your supplied JSON schema. [Learn more in the Structured Outputs guide](#).
    - The default format is `{ "type": "text" }` with no additional options.
    - **Not recommended for gpt-4o and newer models:**  
      Setting to `{ "type": "json_object" }` enables the older JSON mode, which ensures the message the model generates is valid JSON. Using `json_schema` is preferred for models that support it.

    - **Text** (`object`):  
      Default response format. Used to generate text responses.
      - `type` (`string`) **Required**  
        The type of response format being defined. Always `text`.

    - **JSON schema** (`object`):  
      JSON Schema response format. Used to generate structured JSON responses. [Learn more about Structured Outputs](#).
      - `name` (`string`) **Required**  
        The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
      - `schema` (`object`) **Required**  
        The schema for the response format, described as a JSON Schema object. [Learn how to build JSON schemas here](#).
      - `type` (`string`) **Required**  
        The type of response format being defined. Always `json_schema`.
      - `description` (`string`) *Optional*  
        A description of what the response format is for, used by the model to determine how to respond in the format.
      - `strict` (`boolean` or `null`) *Optional*  
        Defaults to `false`.  
        Whether to enable strict schema adherence when generating the output. If set to `true`, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true. To learn more, read the [Structured Outputs guide](#).

    - **JSON object** (`object`):  
      JSON object response format. An older method of generating JSON responses. Using `json_schema` is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so.
      - `type` (`string`) **Required**  
        The type of response format being defined. Always `json_object`.

- `tool_choice` (`string` or `object`) *Optional*  
  How the model should select which tool (or tools) to use when generating a response. See the `tools` parameter to see how to specify which tools the model can call.

- `tools` (`array`) *Optional*  
  An array of tools the model may call while generating a response. You can specify which tool to use by setting the `tool_choice` parameter.

  The two categories of tools you can provide the model are:
  - **Built-in tools:** Tools that are provided by OpenAI that extend the model's capabilities, like web search or file search. [Learn more about built-in tools](#).
  - **Function calls (custom tools):** Functions that are defined by you, enabling the model to call your own code. [Learn more about function calling](#).

- `top_p` (`number` or `null`) *Optional*  
  Defaults to `1`.  
  An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So `0.1` means only the tokens comprising the top 10% probability mass are considered.  
  We generally recommend altering this or `temperature` but not both.

- `truncation` (`string` or `null`) *Optional*  
  Defaults to `disabled`.  
  The truncation strategy to use for the model response.
  - `auto`: If the context of this response and previous ones exceeds the model's context window size, the model will truncate the response to fit the context window by dropping input items in the middle of the conversation.
  - `disabled` (default): If a model response will exceed the context window size for a model, the request will fail with a 400 error.

- `user` (`string`) *Optional*  
  A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](#).

---

### Returns

Returns a **Response** object.

---

### Supported Input Types

- **Text input**
- **Image input**
- **Web search**
- **File search**
- **Streaming**
- **Functions**
- **Reasoning**

---


## Response Object

The response object contains the following fields:

| Field                | Type                | Description                                                                                                                                                                                                                                                                                                                                                      |
|----------------------|---------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `id`                 | string              | Unique identifier for this Response.                                                                                                                                                                                                                                                                                                                            |
| `object`             | string              | The object type of this resource - always set to `response`.                                                                                                                                                                                                                                                                                                     |
| `created_at`         | number              | Unix timestamp (in seconds) of when this Response was created.                                                                                                                                                                                                                                                                                                   |
| `status`             | string              | The status of the response generation. One of `completed`, `failed`, `in_progress`, or `incomplete`.                                                                                                                                                                                                                                                            |
| `error`              | object or null      | An error object returned when the model fails to generate a Response.                                                                                                                                                                                                                                                                                            |
| `incomplete_details` | object or null      | Details about why the response is incomplete.<br> &nbsp;&nbsp;• `reason` (string): The reason why the response is incomplete.                                                                                                                                                                                                                                    |
| `instructions`       | string or null      | Inserts a system (or developer) message as the first item in the model's context. When using along with `previous_response_id`, the instructions from a previous response will not be carried over to the next response. This makes it simple to swap out system (or developer) messages in new responses.                                                        |
| `max_output_tokens`  | integer or null     | An upper bound for the number of tokens that can be generated for a response, including visible output tokens and reasoning tokens.                                                                                                                                                                                                                              |
| `metadata`           | map                 | Set of up to 16 key-value pairs that can be attached to an object. Keys are strings (max 64 chars). Values are strings (max 512 chars). Useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.                                              |
| `model`              | string              | Model ID used to generate the response, like `gpt-4o` or `o3`.                                                                                                                                                                                                                                                           |
| `output`             | array               | An array of content items generated by the model. The length and order of items in the output array is dependent on the model's response. Rather than accessing the first item in the output array and assuming it's an assistant message with the content generated by the model, you might consider using the `output_text` property where supported in SDKs. |
| `output_text`        | string or null      | (SDK Only) SDK-only convenience property that contains the aggregated text output from all `output_text` items in the output array, if any are present. Supported in the Python and JavaScript SDKs.                                                                               |
| `parallel_tool_calls`| boolean             | Whether to allow the model to run tool calls in parallel.                                                                                                                                                                                                                                                                                                        |
| `previous_response_id`| string or null     | The unique ID of the previous response to the model. Use this to create multi-turn conversations.                                                                                                                                                                                                                        |
| `reasoning`          | object or null      | (o-series models only) Configuration options for reasoning models.                                                                                                                                                                                                                                                        |
| `service_tier`       | string or null      | Specifies the latency tier to use for processing the request. See [service tiers](#) for details.                                                                                                                                                                                                                        |
| `temperature`        | number or null      | What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both.                                                                        |
| `text`               | object              | Configuration options for a text response from the model. Can be plain text or structured JSON data.                                                                                                                                                                                                                      |
| `tool_choice`        | string or object    | How the model should select which tool (or tools) to use when generating a response. See the `tools` parameter to see how to specify which tools the model can call.                                                                                                               |
| `tools`              | array               | An array of tools the model may call while generating a response. You can specify which tool to use by setting the `tool_choice` parameter.                                                                                                                                                                               |
| `top_p`              | number or null      | An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So `0.1` means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.      |
| `truncation`         | string or null      | The truncation strategy to use for the model response. <br>• `auto`: If the context of this response and previous ones exceeds the model's context window size, the model will truncate the response to fit the context window by dropping input items in the middle of the conversation.<br>• `disabled` (default): If a model response will exceed the context window size for a model, the request will fail with a 400 error. |
| `usage`              | object              | Represents token usage details including input tokens, output tokens, a breakdown of output tokens, and the total tokens used.                                                                                                                                                                                            |
| `user`               | string              | A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.                                                                                                                                                                                                                        |

### Example

```json
{
  "id": "resp_67ccd3a9da748190baa7f1570fe91ac604becb25c45c1d41",
  "object": "response",
  "created_at": 1741476777,
  "status": "completed",
  "error": null,
  "incomplete_details": null,
  "instructions": null,
  "max_output_tokens": null,
  "model": "gpt-4o-2024-08-06",
  "output": [
    {
      "type": "message",
      "id": "msg_67ccd3acc8d48190a77525dc6de64b4104becb25c45c1d41",
      "status": "completed",
      "role": "assistant",
      "content": [
        {
          "type": "output_text",
          "text": "The image depicts a scenic landscape with a wooden boardwalk or pathway leading through lush, green grass under a blue sky with some clouds. The setting suggests a peaceful natural area, possibly a park or nature reserve. There are trees and shrubs in the background.",
          "annotations": []
        }
      ]
    }
  ],
  "parallel_tool_calls": true,
  "previous_response_id": null,
  "reasoning": {
    "effort": null,
    "summary": null
  },
  "store": true,
  "temperature": 1,
  "text": {
    "format": {
      "type": "text"
    }
  },
  "tool_choice": "auto",
  "tools": [],
  "top_p": 1,
  "truncation": "disabled",
  "usage": {
    "input_tokens": 328,
    "input_tokens_details": {
      "cached_tokens": 0
    },
    "output_tokens": 52,
    "output_tokens_details": {
      "reasoning_tokens": 0
    },
    "total_tokens": 380
  },
  "user": null,
  "metadata": {}
}
```

## List Input Items

**Endpoint:**  
`GET https://api.openai.com/v1/responses/{response_id}/input_items`  
Returns a list of input items for a given response.

---

### Path Parameters

| Name         | Type   | Required | Description                                              |
|--------------|--------|----------|----------------------------------------------------------|
| response_id  | string | Yes      | The ID of the response to retrieve input items for.      |

---

### Query Parameters

| Name     | Type    | Required | Description                                                                                      |
|----------|---------|----------|--------------------------------------------------------------------------------------------------|
| after    | string  | No       | An item ID to list items after, used in pagination.                                              |
| before   | string  | No       | An item ID to list items before, used in pagination.                                             |
| include  | array   | No       | Additional fields to include in the response. See the `include` parameter for Response creation above for more information. |
| limit    | integer | No       | A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20. |
| order    | string  | No       | The order to return the input items in. Default is `asc`. <br> - `asc`: Return the input items in ascending order.<br> - `desc`: Return the input items in descending order. |

---

### Returns

A list of input item objects.

### Example request

```python
from openai import OpenAI
client = OpenAI()

response = client.responses.input_items.list("resp_123")
print(response.data)
```

### OBJECT The response object
```json
{
  "object": "list",
  "data": [
    {
      "id": "msg_abc123",
      "type": "message",
      "role": "user",
      "content": [
        {
          "type": "input_text",
          "text": "Tell me a three sentence bedtime story about a unicorn."
        }
      ]
    }
  ],
  "first_id": "msg_abc123",
  "last_id": "msg_abc123",
  "has_more": false
}
```