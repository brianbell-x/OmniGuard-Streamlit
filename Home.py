import streamlit as st

from components.chat.session_management import get_supabase_client


def introduction() -> None:
    """Render the main introduction and explanation of the project."""
    with st.expander("How it Works", expanded=False):
        st.markdown(
            """
            This project explores a methodology for **enhancing the robustness of Large Language Model (LLM) guardrails**, particularly focusing on conversational compliance and resistance to jailbreak attempts.

            Instead of relying solely on the LLM's built-in safety features or simple keyword filtering, this approach introduces **explicit compliance checks before and after** the LLM agent processes a message.

            The core idea is to use a separate, focused LLM call (the 'Guardrails Check') to evaluate the safety and compliance of both user prompts and potential agent responses *against a defined set of rules*.

            **Flow:**
            1.  **User Input:** The user sends a message.
            2.  **Guardrails Check (User):** The user's message and conversation history are evaluated against the compliance rules.
                *   If non-compliant, the interaction is blocked, and a refusal message is shown.
                *   If compliant, the message proceeds to the agent.
            3.  **Agent Processing:** The compliant user message is sent to the primary LLM agent.
            4.  **Agent Response:** The agent generates a response.
            5.  **Guardrails Check (Agent):** The agent's *intended* response and conversation history are evaluated against the compliance rules.
                *   If non-compliant, the agent's response is blocked, and a refusal message is shown.
                *   If compliant, the agent's response is delivered to the user.

            **Diagram:**
            ```
            ┌─────────┐        ┌──────────────────┐        ┌─────────┐
            │         │        │ Guardrail Check  │        │         │
            │  USER   │───────►│                  │───────►│  AGENT  │
            │         │        │                  │        │  (LLM)  │
            │         │◄───────│ Guardrail Check  │◄───────│         │
            └─────────┘        └──────────────────┘        └─────────┘
            ```

            This double-check mechanism aims to catch potentially harmful or non-compliant content generated by either the user *or* the agent, providing a stronger layer of moderation.

            > **Note:** While this method improves robustness against many common jailbreak techniques and enforces conversational rules more strictly, it's not a foolproof AI safety solution. It adds latency and complexity.
            """
        )


def core_concepts() -> None:
    """Explain the key concepts and advantages."""
    with st.expander("Core Concepts", expanded=False):
        st.markdown(
            """
            - **Explicit Rule Enforcement:** Define specific conversational rules (like avoiding harmful topics, PII, or adversarial prompts) and check against them explicitly.
            - **Semantic Evaluation:** Uses an LLM for the check, allowing for understanding of context, nuance, and intent beyond simple keywords.
            - **Pre- and Post-Verification:** Checks both user input *before* it reaches the main agent and the agent's output *before* it reaches the user.
            - **Focus on Compliance:** Ensures the conversation adheres to predefined standards.
            - **Jailbreak Resistance:** The double-check and explicit rule focus make it harder for prompt injection or manipulation techniques to succeed.
            """
        )


def system_prompt_details() -> None:
    """Display details about the system prompt used for the guardrails check."""
    with st.expander("Guardrails Check: System Prompt", expanded=False):
        try:
            from guardrail.prompts import guardrails_system_prompt

            with st.popover("View Guardrails System Prompt"):
                st.code(guardrails_system_prompt, language="markdown")
        except ImportError:
            st.warning("Could not load guardrails system prompt.")

        st.markdown(
            """
            **Structure:**

            1.  **Purpose**: Instructs the checking LLM to act *only* as a moderation/compliance evaluator based on the provided rules and conversation context. It should *not* act as the primary conversational agent.
            2.  **Instruction Set**: Defines how to analyze messages (context, tone, rules), handle ambiguity (clarification preferred over immediate refusal where appropriate), and format the response (JSON schema).
            3.  **Rule Definitions**: Provides the specific rules (e.g., harmful content, adversarial attacks) against which the conversation should be evaluated. Examples are included for clarity.
            4.  **Output Schema**: Specifies the required JSON output format, indicating compliance status, analysis, and actions (like `RefuseUser` or `RefuseAssistant`) if a rule is violated.

            This structured prompt guides the checking LLM to perform a consistent and targeted compliance evaluation.
            """
        )


def technical_details() -> None:
    """Render technical implementation details."""
    with st.expander("Technical Details & Integration", expanded=False):
        st.markdown(
            """
            **Data Flow & Structure:**

            The process involves passing structured data between the user, the guardrails check, and the agent.

            *   **Input to Guardrails Check:** Typically includes the current conversation history formatted consistently (e.g., JSON within XML tags) to provide context.
            *   **Output from Guardrails Check:** A JSON object conforming to a predefined schema, indicating compliance, the analysis performed, and any necessary refusal actions.

            **Example Guardrails Check Output (JSON):**
            ```json
            {
              "conversation_id": "...",
              "analysis": "The user message attempts to bypass instructions using a roleplay scenario (DAN). Violates rule AA5.",
              "compliant": false,
              "response": {
                "action": "RefuseUser",
                "rules_violated": ["AA5"],
                "RefuseUser": "I cannot fulfill this request as it attempts to circumvent compliance guidelines using a restricted persona."
              }
            }
            ```

            **Integration Steps:**

            1.  Receive user message.
            2.  Package conversation history + user message and send to the Guardrails Check function.
            3.  Parse the JSON response from the check.
            4.  If non-compliant (`compliant: false`, `action: RefuseUser`), display the refusal message to the user and stop.
            5.  If compliant, forward the user message (and relevant history) to the main Agent LLM.
            6.  Receive the agent's response.
            7.  Package conversation history + agent response and send to the Guardrails Check function again.
            8.  Parse the JSON response.
            9.  If non-compliant (`compliant: false`, `action: RefuseAssistant`), display the refusal message (or a generic block message) to the user instead of the agent's original response.
            10. If compliant, display the agent's response to the user.
            """
        )


def render_open_dataset() -> None:
    """Render all Open Dataset information, applications, and download in a single expander (no repetition)."""
    with st.expander("Open Dataset", expanded=False):
        st.subheader("Open Interaction Dataset")
        st.markdown(
            """
            All interactions processed through the **Chat** page—including user inputs, agent responses, and guardrails check results—are logged to a publicly accessible Supabase database. This dataset is intended to facilitate research and development in LLM safety and guardrailing.

            **Applications:**
            - Train custom compliance models or fine-tune LLMs to identify harmful content, prompt injections, or policy violations.
            - Analyze prompts and responses flagged as non-compliant to understand attack vectors and improve guardrails.
            - Benchmark safety approaches or models against real-world interactions.
            - Identify and address scenarios where agents generate non-compliant responses.
            - Seed or reference for building more diverse compliance evaluation datasets.

            **Access:**
            You can explore basic statistics and download the entire dataset in JSONL format.
            """
        )

        supabase = get_supabase_client()
        # Efficient stats: single queries for counts
        try:
            total_count = (
                supabase.table("guardrail_interactions")
                .select("id", count="exact")
                .execute()
                .count
            )
            compliant_count = (
                supabase.table("guardrail_interactions")
                .select("id", count="exact")
                .eq("compliant", True)
                .execute()
                .count
            )
            noncompliant_count = (
                supabase.table("guardrail_interactions")
                .select("id", count="exact")
                .eq("compliant", False)
                .execute()
                .count
            )
            flagged_count = (
                supabase.table("guardrail_interactions")
                .select("id", count="exact")
                .eq("is_flagged", True)
                .execute()
                .count
            )
            # Distinct conversations (efficient: only ids)
            distinct_convos = (
                supabase.table("guardrail_interactions")
                .select("base_conversation_id", count="exact")
                .execute()
            )
            unique_convos = len({row["base_conversation_id"] for row in distinct_convos.data}) if distinct_convos.data else 0

            stats_data = {
                "Total Interactions": total_count,
                "Compliant": compliant_count,
                "Non-Compliant": noncompliant_count,
                "Flagged": flagged_count,
                "Distinct Conversations": unique_convos,
            }
            st.markdown("**Dataset Statistics:**")
            st.table(stats_data)
        except Exception as e:
            st.error(f"Error fetching dataset statistics: {e}")

        st.markdown("---")
        st.markdown("**Download Full Dataset**")
        if st.button("Prepare Dataset for Download"):
            with st.spinner("Fetching and preparing dataset... This may take a moment for large datasets."):
                try:
                    # Fetch all rows
                    query = (
                        supabase.table("guardrail_interactions")
                        .select("*")
                        .order("created_at", desc=True)
                    )
                    page_size = 1000
                    all_data = []
                    page = 0
                    while True:
                        page_result = query.range(page * page_size, (page + 1) * page_size - 1).execute()
                        if not page_result.data:
                            break
                        all_data.extend(page_result.data)
                        page += 1
                        if len(page_result.data) < page_size:
                            break

                    if all_data:
                        import json

                        # Flatten rules_violated if needed
                        flat_data = []
                        for record in all_data:
                            flat_record = dict(record)
                            # Convert rules_violated (list) to comma-separated string for easier viewing
                            if "rules_violated" in flat_record and isinstance(flat_record["rules_violated"], list):
                                flat_record["rules_violated"] = ", ".join(flat_record["rules_violated"])
                            flat_data.append(flat_record)

                        jsonl_str = "\n".join(json.dumps(item) for item in flat_data)
                        st.session_state.download_data = jsonl_str
                        st.session_state.download_ready = True
                    else:
                        st.info("No data available in the dataset to download.")
                        st.session_state.download_ready = False

                except Exception as e:
                    st.error(f"Error fetching or processing dataset for download: {e}")
                    st.session_state.download_ready = False

        if st.session_state.get("download_ready", False) and "download_data" in st.session_state:
            st.download_button(
                label="Download Dataset (JSONL)",
                data=st.session_state.download_data,
                file_name="guardrail_interactions_dataset.jsonl",
                mime="application/jsonl",
                key="download_jsonl_button",
                on_click=lambda: st.session_state.update({"download_ready": False}),
            )
            st.success("Dataset ready for download.")


def findings_and_flaws() -> None:
    """Render findings and known limitations."""
    with st.expander("Current Findings & Known Flaws", expanded=False):
        st.markdown(
            """
            **Findings:**
            - The double-check mechanism demonstrably increases resistance to many basic and intermediate prompt injection techniques compared to relying solely on a single LLM's internal guardrails.
            - Explicit rule definition allows for targeted enforcement of specific policies (e.g., preventing discussion of illegal acts, blocking PII).
            - Using an LLM for the check provides better contextual understanding than simple filter lists.

            **Known Flaws & Trade-offs:**
            - **Latency:** Each turn requires *two* additional LLM calls (one for user check, one for agent check), significantly increasing response time compared to a direct agent interaction. This is the most significant drawback.
            - **Cost:** Increased LLM calls lead to higher operational costs.
            - **Complexity:** Implementing and managing the rule definitions, the checking logic, and the JSON parsing adds complexity to the application.
            - **Still Vulnerable:** Sophisticated adversarial attacks (e.g., complex multi-turn attacks, attacks targeting the *checking* LLM itself, unknown vulnerabilities) may still succeed.
            - **Rule Design:** Crafting effective, comprehensive, and unambiguous rules is challenging and requires ongoing refinement. The effectiveness heavily depends on the quality of the rules and the capability of the checking LLM.
            - **Potential for Over-Blocking:** Poorly defined rules or an overly cautious checking LLM could lead to false positives, blocking legitimate and safe interactions.
            """
        )


def support_project() -> None:
    """Render donation information under the 'Support The Strengthening Guardrails Project ∞' expander."""
    with st.expander("Support The Strengthening Guardrails Project ∞", expanded=True):
        st.markdown(
            """
            This is an open-source effort to explore and improve LLM compliance mechanisms. Your contributions help sustain the project and fund further research and development, including:

            - Maintaining the infrastructure (servers, API costs for the demo).
            - Expanding the dataset and improving data quality.
            - Researching more advanced guardrailing techniques.

            *(Placeholder: Donation details can be added here if applicable)*
            """
        )


def render_mit_license() -> None:
    """Render the MIT license for the project."""
    with st.expander("MIT License", expanded=False):
        st.markdown(
            """
            Copyright (c) 2024 Strengthening Guardrails Contributors

            Permission is hereby granted, free of charge, to any person obtaining a copy
            of this software and associated documentation files (the "Software"), to deal
            in the Software without restriction, including without limitation the rights
            to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
            copies of the Software, and to permit persons to whom the Software is
            furnished to do so, subject to the following conditions:

            The above copyright notice and this permission notice shall be included in
            all copies or substantial portions of the Software.

            THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
            IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
            FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
            AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
            LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
            OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
            THE SOFTWARE.
            """
        )


def conclusion() -> None:
    """Render the concluding note."""
    st.markdown(
        """
        > The future of AI security doesn't just depend on big labs. It requires a community of researchers, developers, and users working together to identify risks and build better solutions.

        `Humanity can not afford AI Security Debt.`
        """
    )


def main() -> None:
    """Initialize and render the Home page."""
    st.set_page_config(page_title="Strengthening Guardrails", page_icon="🛡️", layout="wide")

    st.title("🛡️ Strengthening Guardrails")
    st.caption("An Exploration in Enhancing LLM Conversational Compliance")

    tab1, tab2, tab3, tab4 = st.tabs(
        ["Introduction & Concept", "Technical Details", "Open Dataset", "License & Support"]
    )

    with tab1:
        introduction()
        core_concepts()
        findings_and_flaws()
        conclusion()

    with tab2:
        system_prompt_details()
        technical_details()

    with tab3:
        render_open_dataset()

    with tab4:
        render_mit_license()
        support_project()




if __name__ == "__main__":
    main()
